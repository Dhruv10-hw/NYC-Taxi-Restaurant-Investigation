# -*- coding: utf-8 -*-
"""restaurant_type_dim.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IPfJRnWcap3wpYPdsbR_tRRx8y8Pz65y

***THIS IS A ACTUAL WORKED FILE FOR RESTAURANT DIMENSION***
"""

!pip install sodapy
!pip install credentials
!pip install google-cloud-bigquery sodapy

import pandas as pd
from sodapy import Socrata

data_url = 'data.cityofnewyork.us'
app_token = 'DiYM4Qi0LWQfnpvC11ksbm5ju'

client = Socrata(data_url, app_token)
client.timeout = 240

for x in range(2021, 2025):
    # get data
    start = 0
    chunk_size = 2000
    results = []

    where_clause = f"date_extract_y(inspection_date)={x}"
    data_set = '43nn-pn8j'
    record_count = client.get(data_set, where=where_clause, select='COUNT(*)')
    print(f'Restaurant Inspection data set from {x}')

    while True:
        results.extend(client.get(data_set, where=where_clause, offset=start, limit=chunk_size))
        start += chunk_size
        if (start > int(record_count[0]['COUNT'])):
            break

    # export data to csv
    df = pd.DataFrame.from_records(results)
    df.to_csv("Restaurant_Inspection.csv", index=False)

!pip install ydata-profiling

import pandas as pd
from ydata_profiling import ProfileReport

# Load the dataset
df = pd.read_csv("Restaurant_Inspection.csv")

# Generate the data profiling report
profile = ProfileReport(df, title="Restaurant Inspection Data Profiling Report", explorative=True)

# Save the profiling report to an HTML file
profile.to_file("Restaurant_inspection_data_profiling_report.html")

profile.to_notebook_iframe()

from google.colab import drive
import pandas as pd

# Mount Google Drive
drive.mount('/content/drive')

# Specify the path within your Google Drive
file_path = '/content/drive/My Drive/CIS9350/Restaurant_Inspection.csv'
df = pd.read_csv(file_path)

# Save the DataFrame to a CSV file in the specified path
df.to_csv(file_path, index=False)

print(f"File saved successfully at {file_path}")

# Create the credentials.py file with the correct path
credentials_content = '''path_to_service_account_key_file = "/content/drive/My Drive/CIS9350/my-service-account.json"'''

with open('/content/credentials.py', 'w') as f:
    f.write(credentials_content)

# Verify the file creation
print("credentials.py file created in /content directory.")

import pandas as pd
df = pd.read_csv('Restaurant_Inspection.csv')
print(df.columns)

import credentials

# Commented out IPython magic to ensure Python compatibility.
from google.colab import auth
auth.authenticate_user()
print('Authenticated')


# Google Colab load modules for BigQuery
# %load_ext google.cloud.bigquery
# %load_ext google.colab.data_table

# ETL Complaint Facts
# If using the native Google BigQuery API module:
from google.cloud import bigquery
from google.cloud.exceptions import NotFound
# import credentials
import pandas as pd
import os
import pyarrow
import logging
from datetime import datetime

"""**CREATING FUNCTIONS**"""

def create_bigquery_client(logging):
    try:
        bqclient = bigquery.Client.from_service_account_json("/content/drive/My Drive/CIS9350/my-service-account.json")
        logging.info('BigQuery Client created successfully.')
        return bqclient
    except Exception as err:
        logging.error('Failed to create BigQuery Client.', exc_info=True)
        raise err

def load_csv_data_file(logging, file_source_path, file_name, df):
    try:
        full_path = f"{file_source_path}/{file_name}"
        logging.info(f"Loading file: {full_path}")
        df = pd.read_csv(full_path)
        logging.info(f"Loaded {len(df)} rows from {file_name}.")
        return df
    except FileNotFoundError:
        logging.error(f"File not found: {file_source_path}/{file_name}.")
        raise
    except Exception as e:
        logging.error(f"Error loading file {file_name}: {e}", exc_info=True)

def bigquery_table_exists(bqclient, table_path):
    try:
        bqclient.get_table(table_path)
        return True
    except NotFound:
        return False

def build_new_table(logging, bqclient, table_path, df):
    try:
        logging.info(f"Creating new table: {table_path}")
        #job_config = bigquery.LoadJobConfig(write_disposition="WRITE_TRUNCATE")
        job = bqclient.load_table_from_dataframe(df, table_path) #job_config=job_config)
        job.result()
        logging.info(f"Table {table_path} created with {len(df)} rows.")
    except Exception as e:
        logging.error(f"Failed to create table {table_path}: {e}", exc_info=True)
        raise

def build_new_table(logging, bqclient, table_path, df):
    """
    Creates a new table in BigQuery and loads data from the DataFrame into it.

    Args:
        logging: Logging object.
        bqclient: BigQuery client object.
        table_path: Full path of the table to be created.
        df: Pandas DataFrame to be loaded into the table.

    """
    try:
        # Create the job config
        job_config = bigquery.LoadJobConfig(
            # Set the source format to CSV
            source_format=bigquery.SourceFormat.CSV,
            # Infer schema from dataframe
            autodetect=True,
            # Write disposition WRITE_APPEND if table already exists
            write_disposition="WRITE_APPEND",
            # Create a new table if it does not exist
            create_disposition="CREATE_IF_NEEDED",
            # Use CSV header info
            skip_leading_rows=1,
        )
        # Create the load job
        job = bqclient.load_table_from_dataframe(df, table_path, job_config=job_config)
        job.result()  # Wait for the job to complete
        logging.info(f"Table {table_path} created successfully with {len(df)} rows.")
    except Exception as e:
        logging.error(f"Error creating table {table_path}: {e}", exc_info=True)
        raise  # re-raise error to ensure failures are communicated

def build_new_table(logging, bqclient, table_path, df):
    # Creating table schema from DataFrame
    job_config = bigquery.LoadJobConfig(schema=[
        bigquery.SchemaField("grade", bigquery.enums.SqlTypeNames.STRING),
        bigquery.SchemaField("grade_description", bigquery.enums.SqlTypeNames.STRING),
        bigquery.SchemaField("score_range", bigquery.enums.SqlTypeNames.STRING),
        bigquery.SchemaField("grade_dim_id", bigquery.enums.SqlTypeNames.INTEGER),
    ],
    write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE)

    # Loading data into the table
    job = bqclient.load_table_from_dataframe(
        df, table_path, job_config=job_config
    )  # Make an API request.
    job.result()  # Wait for the job to complete.

    logging.info(f"Created table {table_path} and loaded {len(df)} rows.")

def insert_into_existing_table(logging, bqclient, table_path, surrogate_key, df):
    try:
        logging.info(f"Inserting new records into existing table: {table_path}")
        # Query to fetch existing surrogate keys
        query = f"SELECT {surrogate_key} FROM `{table_path}`"
        existing_keys = bqclient.query(query).to_dataframe()[surrogate_key].tolist()
        # Filter out records already in the table
        df_new = df[~df[surrogate_key].isin(existing_keys)]
        if not df_new.empty:
            job_config = bigquery.LoadJobConfig(write_disposition="WRITE_APPEND")
            job = bqclient.load_table_from_dataframe(df_new, table_path, job_config=job_config)
            job.result()
            logging.info(f"Inserted {len(df_new)} new rows into {table_path}.")
        else:
            logging.info("No new records to insert.")
    except Exception as e:
        logging.error(f"Error inserting new records into {table_path}: {e}", exc_info=True)
        raise

"""**Inspection_Type**"""

def transform_inspection_type_data(logging, df):
    try:
        logging.info("Transforming data for inspection type dimension.")

        # Extract and drop duplicates
        df = df[['inspection_type']].drop_duplicates()

        # Fill missing inspection types with 'Unknown'
        df['inspection_type'] = df['inspection_type'].fillna('Unknown')

        # Categorize inspection types into two broader categories
        def categorize_inspection_type(inspection_type):
            # Define categories for broader inspection types
            if 'Re-inspection' in inspection_type:
                return 'Re-inspection'
            elif 'Inspection' in inspection_type:
                return 'Routine Inspection'
            else:
                return 'Unknown'  # For any unexpected types

        # Apply the categorization function to assign inspection types
        df['inspection_type_category'] = df['inspection_type'].apply(categorize_inspection_type)

        # Map categories to a numerical code (1 for Routine Inspection, 2 for Re-inspection)
        category_mapping = {
            'Routine Inspection': 1,
            'Re-inspection': 2,
            'Unknown': 0  # Default for unclassified types
        }
        df['inspection_category_code'] = df['inspection_type_category'].map(category_mapping)

        # Assign unique IDs for each inspection type
        df['inspection_type_dim_id'] = range(1, len(df) + 1)

        logging.info("Data transformation completed successfully.")
        return df

    except KeyError as e:
        logging.error(f"Missing required columns in the DataFrame: {e}")
        raise
    except Exception as e:
        logging.error(f"Error transforming inspection type data: {e}", exc_info=True)
        raise

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)

    # Parameters
    file_source_path = "/content/drive/My Drive/CIS9350"
    file_name = "Restaurant_Inspection.csv"
    gcp_project = "my-project-0577-434418"
    bq_dataset = "restaurant_inspection"
    table_name = "inspection_type_dimension"
    table_path = f"{gcp_project}.{bq_dataset}.{table_name}"
    surrogate_key = "inspection_type_dim_id"


    df = pd.DataFrame()

    # Load in the data file
    df = load_csv_data_file(logging, file_source_path, file_name, df)

    df = transform_inspection_type_data(logging, df)

    # Create the BigQuery client
    bqclient = create_bigquery_client(logging)

    # See if the target table exists
    if not bigquery_table_exists(bqclient, table_path):
        # Create a new table if it does not exist
        build_new_table(logging, bqclient, table_path, df)
    else:
        # Insert new records if the table alreadyexists
        insert_into_existing_table(logging, bqclient, table_path, surrogate_key, df)

    logging.shutdown()

"""Location_Type"""

def transform_location_data(logging, df):
    try:
        logging.info("Transforming data for location dimension.")
        # Columns for Location dimension
        df = df[['street', 'boro', 'zipcode', 'latitude', 'longitude']]
        # Dropping duplicates
        df = df.drop_duplicates()

        # Handling null values
        df['street'] = df['street'].fillna('Unknown Street')
        df['boro'] = df['boro'].fillna('Unknown Borough')
        df['zipcode'] = df['zipcode'].fillna(99999)  # Replaced missing ZipCode with 99999
        df['latitude'] = df['latitude'].fillna(0.0)
        df['longitude'] = df['longitude'].fillna(0.0)

        # Setting city to "New York" for all rows
        df['city'] = "New York"

         # Reordered the columns to place 'city' after 'boro'
        df = df[['street', 'boro', 'city', 'zipcode', 'latitude', 'longitude']]  # Adjusted column order for granuality

        df['location_dim_id'] = range(1, len(df) + 1)

        logging.info("Data transformation complete.")
        return df
    except KeyError as e:
        logging.error(f"Missing required columns in the DataFrame: {e}")
        raise
    except Exception as e:
        logging.error(f"Error transforming location data: {e}", exc_info=True)
        raise

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)

    # Parameters
    file_source_path = "/content/drive/My Drive/CIS9350"
    file_name = "Restaurant_Inspection.csv"
    gcp_project = "my-project-0577-434418"
    bq_dataset = "restaurant_inspection"
    table_name = "location_dimension"
    table_path = f"{gcp_project}.{bq_dataset}.{table_name}"
    surrogate_key = "location_dim_id"

    df = pd.DataFrame()

    df = load_csv_data_file(logging, file_source_path, file_name, df)

    df = transform_location_data(logging, df)

    bqclient = create_bigquery_client(logging)

    # See if the target table exists
    if not bigquery_table_exists(bqclient, table_path):
        # Create a new table if it does not exist
        build_new_table(logging, bqclient, table_path, df)
    else:
        # Insert new records if the table exists
        insert_into_existing_table(logging, bqclient, table_path, surrogate_key, df)

    logging.shutdown()

"""Grade_Dimension"""

def assign_grade(score):
    """Assigns a letter grade based on the score."""
    if score <= 13:
        return 'A'
    elif score <= 27:
        return 'B'
    elif score >= 28:
        return 'C'
    else:
        return 'Unknown'
def transform_grade_data(logging, df):
    try:
        logging.info("Transforming data for grade dimension.")

        # Assigning grades based on score ranges
        df['grade'] = df['score'].apply(assign_grade)

        # Create grade_description based on grade
        def get_grade_description(grade):
            if grade == 'A':
                return 'Excellent'
            elif grade == 'B':
                return 'Good'
            elif grade == 'C':
                return 'Fair'
            elif grade == 'P':
                return 'Pending'
            elif grade == 'Z':
                return 'Closed'
            else:
                return 'Unknown'  # For any other grade that is not A, B, C, P, or Z

        # Assigning grade descriptions based on the grade
        df['grade_description'] = df['grade'].apply(get_grade_description)

        # Created score ranges as numeric categories
        bins = [0, 13, 27, float('inf')]
        labels = ['0-13', '14-27', '28+']
        df['score_range'] = pd.cut(df['score'], bins=bins, labels=labels, right=True, include_lowest=True)

        df['grade'] = df['grade'].fillna('Unknown')
        df['grade_description'] = df['grade_description'].fillna('No description')
        df['score_range'] = df['score_range'].cat.add_categories('Unknown range').fillna('Unknown range')

        grade_dim = df[['grade', 'grade_description', 'score_range']].drop_duplicates()

        grade_dim['grade_dim_id'] = range(1, len(grade_dim) + 1)

        #created extra column named _index_level_0 and dropping it
        grade_dim = grade_dim.reset_index(drop=True)

        logging.info("Data transformation complete for grade dimension.")
        return grade_dim
    except KeyError as e:
        logging.error(f"Missing required columns in the DataFrame: {e}")
        raise
    except Exception as e:
        logging.error(f"Error transforming grade data: {e}", exc_info=True)
        raise

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    file_source_path = "/content/drive/My Drive/CIS9350"
    file_name = "Restaurant_Inspection.csv"
    gcp_project = "my-project-0577-434418"
    bq_dataset = "restaurant_inspection"
    table_name = "grade_dimension"
    table_path = f"{gcp_project}.{bq_dataset}.{table_name}"
    surrogate_key = "grade_dim_id"


    df = pd.DataFrame()

    df = load_csv_data_file(logging, file_source_path, file_name, df)

    df = transform_grade_data(logging, df)

    bqclient = create_bigquery_client(logging)

     # See if the target table exists
    if not bigquery_table_exists(bqclient, table_path):
        # Create a new table if it does not exist
        build_new_table(logging, bqclient, table_path, df)
    else:
        # Insert new records if the table exists
        insert_into_existing_table(logging, bqclient, table_path, surrogate_key, df)

    logging.shutdown()

"""Violation_Dimension"""

def transform_violation_description_data(logging, df):
    try:
        logging.info("Transforming data for violation description dimension.")

        # Selecting columns
        df = df[['violation_code', 'violation_description']]
        # Strip leading/trailing spaces and remove invisible characters
        #df['violation_code'] = df['violation_code'].apply(lambda x: re.sub(r'\s+', ' ', str(x)).strip())
        #df['violation_description'] = df['violation_description'].apply(lambda x: re.sub(r'\s+', ' ', str(x)).strip())

        #df['violation_code'] = df['violation_code'].str.strip()
        #df['violation_description'] = df['violation_description'].str.strip()

        df = df.drop_duplicates(subset=['violation_code', 'violation_description'])


        df['violation_code'] = df['violation_code'].fillna('Unknown Code')
        df['violation_description'] = df['violation_description'].fillna('No Description')

        df['violation_description_dim_id'] = range(1, len(df) + 1)

        logging.info("Data transformation complete.")
        return df
    except KeyError as e:
        logging.error(f"Missing required columns in the DataFrame: {e}")
        raise
    except Exception as e:
        logging.error(f"Error transforming violation description data: {e}", exc_info=True)
        raise

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)


    file_source_path = "/content/drive/My Drive/CIS9350"
    file_name = "Restaurant_Inspection.csv"
    gcp_project = "my-project-0577-434418"
    bq_dataset = "restaurant_inspection"
    table_name = "violation_description_dimension"
    table_path = f"{gcp_project}.{bq_dataset}.{table_name}"
    surrogate_key = "violation_description_dim_id"

    df = pd.DataFrame()

    df = load_csv_data_file(logging, file_source_path, file_name, df)

    df = transform_violation_description_data(logging, df)

    bqclient = create_bigquery_client(logging)

     # See if the target table exists
    if not bigquery_table_exists(bqclient, table_path):
        # Create a new table if it does not exist
        build_new_table(logging, bqclient, table_path, df)
    else:
        # Insert new records if the table exists
        insert_into_existing_table(logging, bqclient, table_path, surrogate_key, df)

    logging.shutdown()

"""Critical_Flag"""

def transform_critical_flag_data(logging, df):
    try:
        logging.info("Transforming data for Critical Flag dimension.")

        df = df[['critical_flag']].drop_duplicates().reset_index(drop=True)

        df['critical_flag'] = df['critical_flag'].fillna('Unknown')

        df['critical_flag_dim_id'] = range(1, len(df) + 1)

        logging.info("Data transformation complete for Critical Flag.")
        return df
    except KeyError as e:
        logging.error(f"Missing required columns in the DataFrame: {e}")
        raise
    except Exception as e:
        logging.error(f"Error transforming critical flag data: {e}", exc_info=True)
        raise

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)

    # Parameters
    file_source_path = "/content/drive/My Drive/CIS9350"
    file_name = "Restaurant_Inspection.csv"
    gcp_project = "my-project-0577-434418"
    bq_dataset = "restaurant_inspection"
    table_name = "critical_flag_dimension"
    table_path = f"{gcp_project}.{bq_dataset}.{table_name}"
    surrogate_key = "critical_flag_dim_id"

    df = pd.DataFrame()

    df = load_csv_data_file(logging, file_source_path, file_name, df)

    df = transform_critical_flag_data(logging, df)

    bqclient = create_bigquery_client(logging)

     # See if the target table exists
    if not bigquery_table_exists(bqclient, table_path):
        # Create a new table if it does not exist
        build_new_table(logging, bqclient, table_path, df)
    else:
        # Insert new records if the table exists
        insert_into_existing_table(logging, bqclient, table_path, surrogate_key, df)

    logging.shutdown()

"""Restaurant_Type_Dimension"""

def transform_restaurant_type_data(logging, df):
    try:
        logging.info("Transforming data for Restaurant Type dimension.")

        df = df[['cuisine_description']]

        df = df.drop_duplicates()

        df['cuisine_description'] = df['cuisine_description'].fillna('Unknown Cuisine')

        df['restaurant_type_dim_id'] = range(1, len(df) + 1)

        logging.info("Data transformation complete for Restaurant Type.")
        return df
    except KeyError as e:
        logging.error(f"Missing required columns in the DataFrame: {e}")
        raise
    except Exception as e:
        logging.error(f"Error transforming restaurant type data: {e}", exc_info=True)
        raise

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)

    # Parameters
    file_source_path = "/content/drive/My Drive/CIS9350"
    file_name = "Restaurant_Inspection.csv"
    gcp_project = "my-project-0577-434418"
    bq_dataset = "restaurant_inspection"
    table_name = "restaurant_type_dimension"
    table_path = f"{gcp_project}.{bq_dataset}.{table_name}"
    surrogate_key = "restaurant_type_dim_id"

    df = pd.DataFrame()

    df = load_csv_data_file(logging, file_source_path, file_name, df)

    df = transform_restaurant_type_data(logging, df)

    bqclient = create_bigquery_client(logging)

     # See if the target table exists
    if not bigquery_table_exists(bqclient, table_path):
        # Create a new table if it does not exist
        build_new_table(logging, bqclient, table_path, df)
    else:
        # Insert new records if the table exists
        insert_into_existing_table(logging, bqclient, table_path, surrogate_key, df)

    logging.shutdown()

"""**Date_Dimension**

*Creating Dimensions*
"""

def generate_date_dimension(start, end):
    """
    generate_date_dimension
    Generates a DataFrame with date dimension information.
    """
    df = pd.DataFrame({"full_date": pd.date_range(start=start, end=end)})
    df["weekday_name"] = df.full_date.dt.strftime("%A")
    df["month_name"] = df.full_date.dt.strftime("%B")
    df["day_of_month"] = df.full_date.dt.strftime("%d")
    df["month_of_year"] = df.full_date.dt.strftime("%m")
    df["quarter"] = df.full_date.dt.quarter
    df["year"] = df.full_date.dt.strftime("%Y")
    df["date_dim_Id"] = range(1, len(df) + 1)
    return df

def check_for_null_and_duplicates(df):
    """
    Checks for null values and duplicate rows in the DataFrame.
    """
    null_columns = df.isnull().sum()
    if null_columns.any():
        logging.warning(f"Null values found in columns: {null_columns[null_columns > 0]}")
    else:
        logging.info("No null values found.")

#checking for duplicates
    duplicate_rows = df.duplicated().sum()
    if duplicate_rows > 0:
        logging.warning(f"{duplicate_rows} duplicate rows found.")
    else:
        logging.info("No duplicate rows found.")

def build_new_table(logging, bqclient, dimension_table_path, dimension_name, df):
    """
    build_new_table
    Loads a DataFrame into a BigQuery table.
    """
    try:
        job_config = bigquery.LoadJobConfig(write_disposition="WRITE_TRUNCATE")
        job = bqclient.load_table_from_dataframe(df, dimension_table_path, job_config=job_config)
        job.result()  # Wait for the load job to complete
        logging.info(f"Loaded {len(df)} rows into {dimension_table_path}.")
    except Exception as err:
        logging.error(f"Failed to create {dimension_table_path} table.", exc_info=True)
        raise err

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)

    # Define parameters
    gcp_project = "my-project-0577-434418"
    bq_dataset = "restaurant_inspection"
    dimension_name = "date"
    table_name = f"{dimension_name}_dimension"
    dimension_table_path = ".".join([gcp_project, bq_dataset, table_name])

    # Date Dimension DataFrame
    df = generate_date_dimension(start="2021-01-01", end="2024-10-31")

    check_for_null_and_duplicates(df)

    bqclient = create_bigquery_client(logging)

    #  Create a new table if it does not exist
    dataset_ref = bqclient.dataset(bq_dataset)
    try:
        bqclient.get_dataset(dataset_ref)
        print(f"Dataset {bq_dataset} already exists.")
    except NotFound:
        print(f"Dataset {bq_dataset} not found, creating it...")
        dataset = bigquery.Dataset(dataset_ref)
        dataset = bqclient.create_dataset(dataset)
        print(f"Dataset {dataset.dataset_id} created.")

    # Check if the table exists and build it if it doesn't
    if not bigquery_table_exists(bqclient, dimension_table_path):
        build_new_table(logging, bqclient, dimension_table_path, dimension_name, df)
        print(f"Table {dimension_table_path} created successfully.")
    else:
        print(f"Table {dimension_table_path} already exists. Will not overwrite it.")

    logging.shutdown()

