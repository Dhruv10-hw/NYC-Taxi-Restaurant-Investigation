# -*- coding: utf-8 -*-
"""Taxi_Dim.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19oQS8RYTPWDMPv8KZJJLyv5t9_gCVQ5K
"""

!pip install sodapy

import pandas as pd
from sodapy import Socrata

data_url = 'data.cityofnewyork.us'
app_token = 'DiYM4Qi0LWQfnpvC11ksbm5ju'

client = Socrata(data_url, app_token)
client.timeout = 240

for x in range(2021, 2025):
    # get data
    start = 0
    chunk_size = 2000
    results = []

    where_clause = f"complaint_type LIKE 'Taxi Complaint%' AND date_extract_y(created_date)={x}"
    data_set = 'erm2-nwe9'
    record_count = client.get(data_set, where=where_clause, select='COUNT(*)')
    print(f'Taxi Complaint data set from {x}')

    while True:
        results.extend(client.get(data_set, where=where_clause, offset=start, limit=chunk_size))
        start += chunk_size
        if (start > int(record_count[0]['COUNT'])):
            break

    # export data to csv
    df = pd.DataFrame.from_records(results)
    df.to_csv("311_Taxi_Complaint.csv", index=False)

from google.colab import drive
import pandas as pd

# Mount Google Drive
drive.mount('/content/drive')

# Assuming 'results' contains your data
# Create a DataFrame
df = pd.DataFrame.from_records(results)

# Specify the path within your Google Drive
file_path = '/content/drive/My Drive/CIS9350/311_Taxi_Complaint.csv'

# Save the DataFrame to a CSV file in the specified path
df.to_csv(file_path, index=False)

print(f"File saved successfully at {file_path}")

import os
print(os.getcwd())

import os
print(os.path.isfile("311_Taxi_Complaint.csv"))

import pandas as pd

# Load the saved CSV file
df = pd.read_csv("311_Taxi_Complaint.csv")

# Display the first few rows of the data
print(df.head())

!pip install ydata-profiling # Install the ydata-profiling package using pip

import pandas as pd
from ydata_profiling import ProfileReport

# Load the dataset (replace the file path if necessary)
df = pd.read_csv("311_Taxi_Complaint.csv")

# Generate the data profiling report
profile = ProfileReport(df, title="Taxi Complaint Data Profiling Report", explorative=True)

# Save the profiling report to an HTML file
profile.to_file("taxi_complaint_data_profiling_report.html")

# Optionally, you can display the report in a Jupyter notebook (if using Jupyter)
profile.to_notebook_iframe()

pip install google-cloud-bigquery google-auth

from google.colab import drive
import pandas as pd
from google.cloud import bigquery
from google.cloud.exceptions import NotFound
import logging
from datetime import datetime

# Constants for GCP and file paths
gcp_project = 'sodium-lore-442819-k0'
bq_dataset = '311_taxi_complaints'
csv_file_path = '/content/drive/My Drive/CIS9350/311_Taxi_Complaint.csv'
log_file_dir = '/content/drive/My Drive/CIS9350/logs/'

from google.colab import drive
drive.mount('/content/drive')  # Mount Google Drive

# Now specify the path to your key
key_path = '/content/drive/MyDrive/CIS9350/sodium-lore-442819-k0-b67a43235790.json'

from google.cloud import bigquery
from google.oauth2 import service_account

# Construct credentials from the key file
credentials = service_account.Credentials.from_service_account_file(key_path)

# Initialize BigQuery client with explicit credentials
client = bigquery.Client(credentials=credentials, project=credentials.project_id)

# Example: List BigQuery datasets in your project
datasets = client.list_datasets()
for dataset in datasets:
    print(f"Dataset: {dataset.dataset_id}")

def load_csv_data_file(logging, file_name, df):
    """ #load and transform data return back to
    load_csv_data_file
    Accepts a file source path and a file name
    Loads the file into a data frame
    Exits the program on error
    Returns the dataframe
    """
    logging.info(f'Reading source data file: {file_name}')
    try:
        df = pd.read_csv(file_name, low_memory=False)
        df = df.rename(columns=str.lower)
        logging.info(f'Read {len(df)} records from source data file: {file_name}')
        return df
    except Exception as e:
        logging.error(f'Failed to read file: {file_name}. Error: {str(e)}')
        raise

def load_csv_data_file(logging, file_path, df):
    try:
        df = pd.read_csv(file_path)  # Correct way to load CSV data into a DataFrame
        logging.info(f"Loaded {len(df)} records from {file_path}.")
    except Exception as e:
        logging.error(f"Error loading CSV file {file_path}: {e}")
    return df

def transform_data(logging, df, columns): # Add columns as an argument
    """
    transform_data
    Accepts a data frame
    Performs any specific cleaning and transformation steps on the dataframe
    Returns the modified dataframe
    """
    logging.info('Transforming dataframe.')
    df = df[columns] # Select columns
    df = df.drop_duplicates()
    if not isinstance(df, pd.DataFrame):
        df = pd.DataFrame(df)
    return df

def transform_data(logging, columns, df):
    """
    transform_data
    Accepts a data frame
    Performs any specific cleaning and transformation steps on the dataframe
    Returns the modified dataframe
    """
    logging.info('Transforming dataframe.')

    # Ensure `columns` is a list of column names, and filter the dataframe
    if isinstance(columns, list):
        df = df[columns]  # Access columns in DataFrame by name
    else:
        logging.warning("Expected 'columns' to be a list of column names.")
        return df

    # Remove duplicates if any
    df = df.drop_duplicates()

    # Ensure df is a DataFrame (if the input is not already a DataFrame)
    if not isinstance(df, pd.DataFrame):
        df = pd.DataFrame(df)

    return df

from google.cloud import bigquery
from google.oauth2 import service_account
import logging
import os

def create_bigquery_client(logging=None):
    """
    Creates a BigQuery client using the service account key from Google Drive.
    Returns the BigQuery client object or None if creation fails.
    """
    # Use environment variable or hardcoded path for the key file
    key_path = os.getenv('BIGQUERY_KEY_PATH', '/content/drive/MyDrive/CIS9350/sodium-lore-442819-k0-b67a43235790.json')

    try:
        # Construct credentials from the key file
        credentials = service_account.Credentials.from_service_account_file(key_path)

        # Initialize BigQuery client with credentials
        bqclient = bigquery.Client(credentials=credentials, project=credentials.project_id)
        if logging:
        # Log client creation
          logging.info('Created BigQuery client: %s', bqclient)

        return bqclient

    except Exception as err:
        # Log error with exception details
        if logging:
          logging.error('Failed to create BigQuery client.', exc_info=True)
        return None

bqclient = create_bigquery_client(None)

bqclient.create_dataset('311_taxi_complaints',exists_ok=True)

def upload_bigquery_table(logging, bqclient, table_path, write_disposition, df):
    """
    upload_bigquery_table
    Accepts a path to a BigQuery table, the write disposition and a dataframe
    Loads the data into the BigQuery table from the dataframe.
    for credentials.
    The write disposition is either
    write_disposition="WRITE_TRUNCATE"  Erase the target data and load all new data.
    write_disposition="WRITE_APPEND"    Append to the existing table
    """
    try:
        logging.info('Creating BigQuery Job configuration with write_disposition=%s', write_disposition)
        job_config = bigquery.LoadJobConfig(write_disposition=write_disposition)
        logging.info('Submitting the BigQuery job')
        job = bqclient.load_table_from_dataframe(df, table_path, job_config=job_config)
        logging.info('Job  results: %s',job.result())
    except Exception as err:
        logging.error('Failed to load BigQuery Table. %s', err)

def bigquery_table_exists(bqclient, table_path):
    """
    bigquery_table_exists
    Accepts a path to a BigQuery table
    Checks if the BigQuery table exists.
    Returns True or False
    """
    try:
        bqclient.get_table(table_path)
        return True
    except NotFound:
        return False

def query_bigquery_table(logging, table_path, bqclient, surrogate_key):
    """
    query_bigquery_table
    Accepts a path to a BigQuery table and the name of the surrogate key
    Queries the BigQuery table but leaves out the update_timestamp and surrogate key columns
    Returns the dataframe
    """
    bq_df = pd.DataFrame
    sql_query = 'SELECT * EXCEPT ( update_timestamp, ' + surrogate_key + ') FROM `' + table_path + '`'
    logging.info('Running query: %s', sql_query)
    try:
        bq_df = bqclient.query(sql_query).to_dataframe()
    except Exception as err:
        logging.info('Error querying the table. %s', err)
    return bq_df

def add_surrogate_key(df, dimension_name, offset=1):
    """
    add_surrogate_key
    Accepts a data frame and inserts an integer identifier as the first column
    Returns the modified dataframe
    """
    df.reset_index(drop=True, inplace=True)
    df.insert(0, dimension_name + '_dim_id', df.index + offset)
    return df

def add_update_date(df, current_date):
    """
    add_update_date
    Accepts a data frame and inserts the current date as a new field
    Returns the modified dataframe
    """
    df['update_date'] = pd.to_datetime(current_date)
    return df

def add_update_timestamp(df):
    """
    add_update_timestamp
    Accepts a data frame and inserts the current datetime as a new field
    Returns the modified dataframe
    """
    df['update_timestamp'] = pd.Timestamp('now', tz='utc').replace(microsecond=0)
    return df

def build_new_table(logging, bqclient, dimension_table_path, dimension_name, df):
    """
    build_new_table
    Accepts a path to a dimensional table, the dimension name and a data frame
    Add the surrogate key and a record timestamp to the data frame
    Inserts the contents of the dataframe to the dimensional table.
    """
    logging.info('Target dimension table %s does not exit', dimension_table_path)

    if df is not None and not df.empty:
        df = add_surrogate_key(df, dimension_name, 1)
        df = add_update_timestamp(df)
        upload_bigquery_table(logging, bqclient, dimension_table_path, 'WRITE_TRUNCATE', df)
    else:
        logging.warning('No data to insert into the new table.')

def insert_existing_table(logging, bqclient, dimension_table_path, dimension_name, surrogate_key, df):
    """
    insert_existing_table
    Accepts a path to a dimensional table, the dimension name and a data frame
    Compares the new data to the existing data in the table.
    Inserts the new/modified records to the existing table
    """
    bq_df = pd.DataFrame()
    logging.info('Target dimension table %s exits. Checking for differences.', dimension_table_path)
    bq_df = query_bigquery_table(logging, dimension_table_path, bqclient, surrogate_key)
    new_records_df = df[~df.apply(tuple,1).isin(bq_df.apply(tuple,1))]
    logging.info('Found %d new records.', new_records_df.shape[0])
    if new_records_df.shape[0] > 0:
        new_surrogate_key_value = bq_df.shape[0] + 1
        new_records_df = add_surrogate_key(new_records_df, dimension_name, new_surrogate_key_value)
        new_records_df = add_update_timestamp(new_records_df)
        upload_bigquery_table(logging, bqclient, dimension_table_path, 'WRITE_APPEND', new_records_df)

"""**Dimension Tables**

**311 Taxi Complaints Dataset**
"""

import os
# Check if the file exists
print(os.path.exists('/content/drive/My Drive/CIS9350/311_Taxi_Complaint.csv'))

#gcp_project = 'sodium-lore-442819-k0'
#bq_dataset = '311_taxi_complaints'
#dimension_table_path = f'{gcp_project}.taxi_and_restaurant_dataset.{table_name}'

df1 = pd.read_csv('311_Taxi_Complaint.csv')
print(df1.columns)

#ls /content/drive/My\ Drive/CIS9350/

import os
import logging
import pandas as pd
import gdown
from datetime import datetime
from google.cloud import bigquery  # Make sure the BigQuery client library is installed

# Constants for GCP and file paths
gcp_project = 'sodium-lore-442819-k0'
bq_dataset = '311_taxi_complaints'
csv_file_path = '/content/drive/My Drive/CIS9350/311_Taxi_Complaint.csv'
log_file_dir = '/content/drive/My Drive/CIS9350/logs/'

# Dimension dictionary for Taxi Complaints
dim_dict = {
    'complaint_type': ['complaint_type', 'descriptor'],
    'location': ['location_type', 'street_name', 'city', 'borough', 'incident_zip', 'longitude', 'latitude'],
    'agency': ['agency', 'agency_name'],
    'complaints_status': ['status'],
}

import pandas as pd
import numpy as np

def fill_nan_with_unknown_or_zero(df):
    """Fills NaN values with 'unknown' for string columns and 0 for numeric columns."""
    for col in df.columns:
        if df[col].dtype == np.dtype('O'):  # Check if column is of object type (string)
            df[col].fillna('unknown', inplace=True)
        else:
            df[col].fillna(0, inplace=True)  # Fill with 0 for numeric columns
    return df

# Load the dataset
df = pd.read_csv('311_Taxi_Complaint.csv')

# Apply the custom function
df = fill_nan_with_unknown_or_zero(df)

from google.colab import auth
auth.authenticate_user()

# Main ETL process for each dimension
def process_etl(logging, dimension_name, file_path, columns, dimension_table_path):
    """ETL process for a specific dimension."""
    try:
        # Initialize the dataframe
        df = pd.DataFrame()

        # Load and transform data
        df = load_csv_data_file(logging, file_path, df)

        if df.empty:
            logging.warning(f"No data found in {file_path}. Skipping dimension {dimension_name}.")
            return

        df = transform_data(logging, columns, df)
        # Create BigQuery client
        bqclient = create_bigquery_client(logging)
        target_table_exists = bigquery_table_exists(bqclient, dimension_table_path)
        # Load data into BigQuery
        if not target_table_exists:
            build_new_table(logging, bqclient, dimension_table_path, dimension_name, df)  # Pass df as an argument
        else:
          # Set the name of the surrogate key
            surrogate_key = f"{dimension_name}_dim_id"
            insert_existing_table(logging, bqclient, dimension_table_path,dimension_name, surrogate_key, df)

        logging.info(f"Successfully processed dimension {dimension_name}.")
    except Exception as e:
        logging.error(f"Error processing dimension {dimension_name}: {e}")

# Main loop to run ETL for all dimensions
def run_etl():
    for key, value in dim_dict.items():
        dimension_name = key
        columns = value
        table_name = f'{dimension_name}_dimension'
        dimension_table_path = f'{gcp_project}.{bq_dataset}.{table_name}'

        # Process the data for the specific dimension
        process_etl(logging, dimension_name, csv_file_path, columns, dimension_table_path)

        logging.info(f"ETL process completed for dimension {dimension_name}.")
    logging.shutdown()

# Run the ETL process
if __name__ == "__main__":
    run_etl()

import os

# Directory where the logs are stored
log_directory = '/content/drive/My Drive/CIS9350/logs/'

# List all log files
log_files = [f for f in os.listdir(log_directory) if f.endswith('.log')]

# Print file details (simulating 'ls -l')
for log_file in log_files:
    file_path = os.path.join(log_directory, log_file)
    file_stats = os.stat(file_path)
    print(f"{log_file} - Size: {file_stats.st_size} bytes - Last Modified: {datetime.fromtimestamp(file_stats.st_mtime)}")

!tail -35 "/content/drive/My Drive/CIS9350/logs/etl_complaint_type_20241126.log"

!tail -35 "/content/drive/My Drive/CIS9350/logs/etl_location_20241126.log"

"""**DATE DIMENSION**"""

def generate_date_dimension(start, end):
    """
    generate_date_dimension
    Generates a DataFrame with date dimension information.
    """
    df = pd.DataFrame({"full_date": pd.date_range(start=start, end=end)})
    df["weekday_name"] = df.full_date.dt.strftime("%A")
    df["month_name"] = df.full_date.dt.strftime("%B")
    df["day_of_month"] = df.full_date.dt.strftime("%d")
    df["month_of_year"] = df.full_date.dt.strftime("%m")
    df["quarter"] = df.full_date.dt.quarter
    df["year"] = df.full_date.dt.strftime("%Y")
    df["date_dim_Id"] = range(1, len(df) + 1)
    return df

def generate_date_dimension(start, end):
    """
    generate_date_dimension
    Generates a DataFrame with date dimension information.
    """
    df = pd.DataFrame({"full_date": pd.date_range(start=start, end=end)})
    df["weekday_name"] = df.full_date.dt.strftime("%A")
    df["month_name"] = df.full_date.dt.strftime("%B")
    df["day_of_month"] = df.full_date.dt.strftime("%d")
    df["month_of_year"] = df.full_date.dt.strftime("%m")
    df["quarter"] = df.full_date.dt.quarter
    df["year"] = df.full_date.dt.strftime("%Y")
    df["date_dim_id"] = range(1, len(df) + 1)
    return df

def check_for_null_and_duplicates(df):
    """
    Checks for null values and duplicate rows in the DataFrame.
    """
    null_columns = df.isnull().sum()
    if null_columns.any():
        logging.warning(f"Null values found in columns: {null_columns[null_columns > 0]}")
    else:
        logging.info("No null values found.")

#checking for duplicates
    duplicate_rows = df.duplicated().sum()
    if duplicate_rows > 0:
        logging.warning(f"{duplicate_rows} duplicate rows found.")
    else:
        logging.info("No duplicate rows found.")

def build_new_table(logging, bqclient, dimension_table_path, dimension_name, df):
    """
    build_new_table
    Loads a DataFrame into a BigQuery table.
    """
    try:
        job_config = bigquery.LoadJobConfig(write_disposition="WRITE_TRUNCATE")
        job = bqclient.load_table_from_dataframe(df, dimension_table_path, job_config=job_config)
        job.result()  # Wait for the load job to complete
        logging.info(f"Loaded {len(df)} rows into {dimension_table_path}.")
    except Exception as err:
        logging.error(f"Failed to create {dimension_table_path} table.", exc_info=True)
        raise err

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)

    # Define parameters
    gcp_project = "sodium-lore-442819-k0"
    bq_dataset = "311_taxi_complaints"
    dimension_name = "date"
    table_name = f"{dimension_name}_dimension"
    dimension_table_path = ".".join([gcp_project, bq_dataset, table_name])

    # Date Dimension DataFrame
    df = generate_date_dimension(start="2021-01-01", end="2024-10-31")

    check_for_null_and_duplicates(df)

    bqclient = create_bigquery_client()

    #  Create a new table if it does not exist
    dataset_ref = bqclient.dataset(bq_dataset)
    try:
        bqclient.get_dataset(dataset_ref)
        print(f"Dataset {bq_dataset} already exists.")
    except NotFound:
        print(f"Dataset {bq_dataset} not found, creating it...")
        dataset = bigquery.Dataset(dataset_ref)
        dataset = bqclient.create_dataset(dataset)
        print(f"Dataset {dataset.dataset_id} created.")

    # Check if the table exists and build it if it doesn't
    if not bigquery_table_exists(bqclient, dimension_table_path):
        build_new_table(logging, bqclient, dimension_table_path, dimension_name, df)
        print(f"Table {dimension_table_path} created successfully.")
    else:
        print(f"Table {dimension_table_path} already exists. Will not overwrite it.")

    logging.shutdown()

"""**TIME DIMENSION**"""

import pandas as pd

def create_time_dimension(start_date, end_date):
    """
    Creates a Pandas DataFrame with time dimension columns.

    Args:
        start_date (str): Start date in YYYY-MM-DD format.
        end_date (str): End date in YYYY-MM-DD format.

    Returns:
        pandas.DataFrame: A DataFrame with time dimension columns.
    """

    date_range = pd.date_range(start=start_date, end=end_date)
    df = pd.DataFrame(date_range, columns=['Date'])

    df['Year'] = df['Date'].dt.year
    df['Quarter'] = df['Date'].dt.quarter
    df['Month'] = df['Date'].dt.month
    df['Day'] = df['Date'].dt.day
    df['DayOfWeek'] = df['Date'].dt.dayofweek
    df['DayName'] = df['Date'].dt.day_name()
    df['Hour'] = df['Date'].dt.hour
    df['Minute'] = df['Date'].dt.minute
    df['Second'] = df['Date'].dt.second
    df['WeekOfYear'] = df['Date'].dt.isocalendar().week

    return df

def load_to_bigquery(df, project_id, dataset_id, table_id):
    """
    Loads a Pandas DataFrame to a BigQuery table.

    Args:
        df (pandas.DataFrame): The DataFrame to load.
        project_id (str): The Google Cloud Project ID.
        dataset_id (str): The BigQuery dataset ID.
        table_id (str): The BigQuery table ID.
    """

    client = bigquery.Client(project=project_id)
    # Create a fully-qualified table ID without redundant project ID in dataset_id
    table_ref = f"{project_id}.{dataset_id.split('.')[-1]}.{table_id}"  # Change is here
    job_config = bigquery.LoadJobConfig(
        write_disposition="WRITE_TRUNCATE"
    )
    job = client.load_table_from_dataframe(df, table_ref, job_config=job_config)
    job.result()


if __name__ == "__main__":
    # Replace with your project ID, dataset ID, and table name
    project_id = "sodium-lore-442819-k0"
    dataset_id = "sodium-lore-442819-k0.311_taxi_complaints"  # Includes project ID
    table_id = "time_dimension"

    # Create the time dimension DataFrame
    time_dim_df = create_time_dimension("2021-01-01", "2024-10-31")

    # Load the DataFrame to BigQuery
    load_to_bigquery(time_dim_df, project_id, dataset_id, table_id)

import matplotlib.pyplot as plt

# Group complaints by year
df['created_year'] = pd.to_datetime(df['created_date']).dt.year
complaints_by_year = df['created_year'].value_counts()

# Plot
complaints_by_year.sort_index().plot(kind='bar', color='skyblue')
plt.title('Taxi Complaints by Year')
plt.xlabel('Year')
plt.ylabel('Number of Complaints')
plt.show()